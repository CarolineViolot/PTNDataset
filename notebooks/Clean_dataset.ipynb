{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0000-imports",
   "metadata": {},
   "source": [
    "# Dataset Cleaning Pipeline\n",
    "**channels:**\n",
    "1. Load Excel source files and filter channels (>500 subs, right channel types, valid `mis` flag)\n",
    "2. Merge and save channel JSON files\n",
    "3. Verify channels dataset integrity\n",
    "4. Save final channels dataset\n",
    "\n",
    "**posts:**\n",
    "\n",
    "5. Check and fix post JSON filenames\n",
    "6. Merge all post JSONs into one file\n",
    "7. Clean post fields\n",
    "8. Pseudonymize commenters\n",
    "9. Verify media hash–to–media_id mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0001-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "if 'notebooks' in os.listdir('../'):\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000-channels-header",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1 — CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001-load-excel",
   "metadata": {},
   "source": [
    "## Step 1 - Load Excel source files and filter channels\n",
    "Keep only channels with **>500 subscribers** from `TGStat_merged.xlsx` (news + politics) and exclude those classified as not politics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1003-load-news",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396 news channels with more than 500 subs\n"
     ]
    }
   ],
   "source": [
    "# Load TGStat news file and show channels with >=500 subs\n",
    "TGstat_news = pd.read_excel(\"../data/TGstat_results_news.xlsx\")\n",
    "print(f\"{len(TGstat_news[TGstat_news.n_sub >= 500])} news channels with more than 500 subs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1004-load-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TGStat politics file\n",
    "TGstat_results_politics = pd.read_excel('../data/TGstat_results_politics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1005-count-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 political channels to keep\n",
      "191 political channels to exclude\n"
     ]
    }
   ],
   "source": [
    "# Count valid politics channels (exclude == 0 means keep)\n",
    "print(f\"{len(TGstat_results_politics[TGstat_results_politics.exclude == 0])} political channels to keep\")\n",
    "political_channels_to_exclude = list(TGstat_results_politics[TGstat_results_politics.exclude == 1].handle)\n",
    "print(f\"{len(political_channels_to_exclude)} political channels to exclude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1006-load-channel-types",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instant_news' 'politics' 'official_in' 'official_out' nan]\n"
     ]
    }
   ],
   "source": [
    "# Load Amir's channel classification file (with channel_type column)\n",
    "channels_final_dataset_w_channel_type = pd.read_excel('../data/channels_all_classified_by_Amir.xlsx')\n",
    "print(channels_final_dataset_w_channel_type.channel_type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12b7cfc7-2aa7-4058-8f4b-aaee0ccbed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset should have a defined channel_type (not nan) and not be excluded out of politics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0870326-4099-42f9-96b0-c7fee3edf11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_final_dataset_w_channel_type = channels_final_dataset_w_channel_type[\n",
    "    ~channels_final_dataset_w_channel_type['channel_type'].isna()]\n",
    "channels_final_dataset_w_channel_type = channels_final_dataset_w_channel_type[\n",
    "    ~channels_final_dataset_w_channel_type['url'].isin(political_channels_to_exclude)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c550d6b-ac76-49f6-bd41-89de43bb4816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2190"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(channels_final_dataset_w_channel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1007-save-channel-types",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-save with clean index\n",
    "channels_final_dataset_w_channel_type.to_excel('../data/channels_final_dataset_w_channel_type.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000-merge-json",
   "metadata": {},
   "source": [
    "## Step 2 - Load and merge channel JSON files\n",
    "Read `TGstat_channels.json` and `recommended_channels.json`, tag each with a `type` field, concatenate, and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2001-load-jsons",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Telegram_data/metadata/TGstat_channels.json\") as f:\n",
    "    TGstat_channels = json.load(f)\n",
    "with open(\"../data/Telegram_data/metadata/recommended_channels.json\") as f:\n",
    "    recommended_channels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2002-tag-type",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag each channel with its source type\n",
    "for i, _ in enumerate(TGstat_channels):\n",
    "    TGstat_channels[i][\"source\"] = \"TGstat\"\n",
    "for i, _ in enumerate(recommended_channels):\n",
    "    recommended_channels[i][\"source\"] = \"recommended\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2003-concat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total channels: 2335  (TGstat: 1741, recommended: 594)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 1455521249,\n",
       " 'title': 'کانال خبری پدافند',\n",
       " 'username': 'Modafeane_Aja',\n",
       " 'broadcast': True,\n",
       " 'verified': False,\n",
       " 'restricted': False,\n",
       " 'scam': False,\n",
       " 'fake': False,\n",
       " 'access_hash': '4117757331980452103',\n",
       " 'date': '2020-03-17T10:01:51+00:00',\n",
       " 'about': 'ما را در شبکه های اجتماعی دنبال کنید\\n https://padafandnews.com\\n https://instagram.com/padafand_aja',\n",
       " 'subscribers': 1231,\n",
       " 'channel_url': 'modafeane_aja',\n",
       " 'restriction_reason': None,\n",
       " 'type': 'recommended'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = TGstat_channels + recommended_channels\n",
    "print(f\"Total channels: {len(channels)}  (TGstat: {len(TGstat_channels)}, recommended: {len(recommended_channels)})\")\n",
    "channels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004-save-all-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Telegram_data/metadata/all_channels_w_all_info.json\", 'w') as f:\n",
    "    json.dump(channels, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000-verify",
   "metadata": {},
   "source": [
    "## Step 3 - Verify data integrity\n",
    "Check for duplicates and missing values in the filtered channel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4001-reload",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels.json length:\t 2198\n",
      "unique username:\t 2159\n",
      "unique channel_url:\t 2198\n",
      "unique id:\t\t 2191\n",
      "channels with id=NaN:\t 8\n"
     ]
    }
   ],
   "source": [
    "channels_df = pd.read_json(\"../data/Telegram_data/metadata/all_channels_w_all_info.json\")\n",
    "channels_df['id'] = (channels_df['id'].astype('Int64').astype(str).replace(\"<NA>\", np.nan))\n",
    "\n",
    "print(\"channels.json length:\\t\", len(channels_df))\n",
    "print(\"unique username:\\t\", len(channels_df['username'].unique()))\n",
    "print(\"unique channel_url:\\t\", len(channels_df['channel_url'].unique()))\n",
    "print(\"unique id:\\t\\t\", len(channels_df['id'].unique()))\n",
    "print(\"channels with id=NaN:\\t\", len(channels_df[channels_df['id'].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4002-verify-no-dup-ids",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify: no duplicated channel ids, except those we could not retrieve (id = nan) ✅\n",
    "channels_df[channels_df.duplicated(subset=['id'], keep=False)]['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4003-verify-no-dup-urls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify: no duplicated channel_urls ✅\n",
    "channels_df[channels_df.duplicated(subset=['channel_url'], keep=False)]['channel_url'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4004-verify-no-title",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify: channels with no title are those we could not retrieve (id always NaN too) ✅\n",
    "channels_df[channels_df['title'].isna()]['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4005-verify-no-subs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify: channels with no subscribers are those we could not retrieve ✅\n",
    "channels_df[channels_df.subscribers.isna()]['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4006-verify-fields",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcast values: [ 1. nan]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>username</th>\n",
       "      <th>broadcast</th>\n",
       "      <th>verified</th>\n",
       "      <th>restricted</th>\n",
       "      <th>scam</th>\n",
       "      <th>fake</th>\n",
       "      <th>access_hash</th>\n",
       "      <th>date</th>\n",
       "      <th>about</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>channel_url</th>\n",
       "      <th>restriction_reason</th>\n",
       "      <th>type</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, username, broadcast, verified, restricted, scam, fake, access_hash, date, about, subscribers, channel_url, restriction_reason, type, error]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scam values: [ 0. nan]\n",
      "fake values: [ 0. nan]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>username</th>\n",
       "      <th>broadcast</th>\n",
       "      <th>verified</th>\n",
       "      <th>restricted</th>\n",
       "      <th>scam</th>\n",
       "      <th>fake</th>\n",
       "      <th>access_hash</th>\n",
       "      <th>date</th>\n",
       "      <th>about</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>channel_url</th>\n",
       "      <th>restriction_reason</th>\n",
       "      <th>type</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, username, broadcast, verified, restricted, scam, fake, access_hash, date, about, subscribers, channel_url, restriction_reason, type, error]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restricted values: [ 0.  1. nan]\n",
      "['This channel can’t be displayed because it violated local laws (France).', 'This channel can’t be displayed because it violated local laws (France).', 'This channel can’t be displayed because it violated local laws (France).', 'This channel can’t be displayed because it violated local laws (France).', 'This channel can’t be displayed because it violated local laws (France).']\n",
      "5 channels were restricted because they violated local laws.\n",
      "verified — True: 39 / False: 2151\n"
     ]
    }
   ],
   "source": [
    "# Verify the boolean fields we can safely drop\n",
    "\n",
    "# broadcast — should be all True (broadcast-only channels)\n",
    "print(\"broadcast values:\", channels_df['broadcast'].unique())\n",
    "display(channels_df[channels_df['broadcast'] == 0])  # expect empty\n",
    "\n",
    "# scam — should be all False\n",
    "print(\"scam values:\", channels_df['scam'].unique())\n",
    "\n",
    "# fake — should be all False\n",
    "print(\"fake values:\", channels_df['fake'].unique())\n",
    "display(channels_df[channels_df['fake'] == 1])\n",
    "\n",
    "# restricted — some channels restricted for local law violations\n",
    "print(\"restricted values:\", channels_df['restricted'].unique())\n",
    "restricted = channels_df[channels_df['restricted'] == 1]\n",
    "if len(restricted):\n",
    "    print([r[0]['text'] for r in restricted['restriction_reason'].values])\n",
    "    print(f\"{len(restricted)} channels were restricted because they violated local laws.\")\n",
    "\n",
    "# verified\n",
    "print(\"verified — True:\", len(channels_df[channels_df['verified'] == 1]),\n",
    "      \"/ False:\", len(channels_df[channels_df['verified'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000-final-channel-dataset",
   "metadata": {},
   "source": [
    "## Step 4 - Prepare and save final channel dataset\n",
    "Keep only the useful columns, merge the Amir channel-type classification, rename columns, and export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5001-drop-na-ids",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2198\n",
      "2190\n"
     ]
    }
   ],
   "source": [
    "# Drop channels we could not retrieve (id = NaN)\n",
    "print(len(channels_df))\n",
    "channels_df = channels_df.dropna(subset='id')\n",
    "print(len(channels_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5002-select-columns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant columns (drop broadcast, scam, fake, access_hash, username, restriction_reason, error)\n",
    "channels_df = channels_df[['id', 'title', 'channel_url', 'date', 'about', 'subscribers', 'type', 'verified', 'restricted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5003-merge-channel-type",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channel_type from Amir's classification\n",
    "channels_final_dataset_w_channel_type['id'] = channels_final_dataset_w_channel_type['id'].apply(str)\n",
    "channels_df = channels_df.merge(channels_final_dataset_w_channel_type[['id', 'channel_type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5004-rename",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for clarity\n",
    "channels_df.rename(columns={'channel_url': 'url', 'type': 'source'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5005-save-final-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2190 channels to channels_final_dataset.json / .csv\n"
     ]
    }
   ],
   "source": [
    "# Export final channel dataset\n",
    "channels_df.to_json('../data/Telegram_data/metadata/channels_final_dataset.json',\n",
    "                    force_ascii=False, orient='records', indent=2)\n",
    "channels_df.to_csv('../data/Telegram_data/metadata/channels_final_dataset.csv')\n",
    "print(f\"Saved {len(channels_df)} channels to channels_final_dataset.json / .csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000-posts-header",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2 — POSTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001-load-channels-for-posts",
   "metadata": {},
   "source": [
    "## Step 5 - Check and fix post JSON filenames, remove posts from removed channels\n",
    "Make sure each `telegram_messages-<handle>.json` file is named after the canonical `channel_url`, not a former username or title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6002-reload-channels-for-posts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2190"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload channel metadata (fresh copy with all channels)\n",
    "channels_df = pd.read_json(\"../data/Telegram_data/metadata/all_channels_w_all_info.json\")\n",
    "channels_df['id'] = (channels_df['id'].astype('Int64').astype(str).replace(\"<NA>\", np.nan))\n",
    "channels_df = channels_df.dropna(subset='id')\n",
    "len(channels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6003-list-post-files",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2190 post files found\n"
     ]
    }
   ],
   "source": [
    "channel_filenames = [f.replace('telegram_messages-', '').replace('.json', '')\n",
    "                     for f in os.listdir('../data/Telegram_data/posts') if \".json\" in f]\n",
    "print(f\"{len(channel_filenames)} post files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6004-lookup-dicts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lookup dictionaries\n",
    "username2handle = dict(zip(channels_df['username'], channels_df['channel_url']))\n",
    "title2handle    = dict(zip(channels_df['title'],    channels_df['channel_url']))\n",
    "sender_id2handle = dict(zip(channels_df['id'],      channels_df['channel_url']))\n",
    "\n",
    "def get_sender_id(filename):\n",
    "    with open(f\"../data/Telegram_data/posts/telegram_messages-{filename}.json\") as f:\n",
    "        data = json.load(f)\n",
    "    if len(data) > 0:\n",
    "        return int(str(data[0]['sender_id']).replace('-100', ''))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6005-detect-wrong-names",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed_names: 0\n",
      "incorrect_names: 0\n"
     ]
    }
   ],
   "source": [
    "incorrect_names = []\n",
    "changed_names   = []\n",
    "\n",
    "for f in channel_filenames:\n",
    "    f_ = username2handle.get(f, f)\n",
    "    f_ = title2handle.get(f_, f_)\n",
    "    if f_ not in list(channels_df['channel_url']):\n",
    "        incorrect_names.append(f_)\n",
    "    elif f != f_:\n",
    "        changed_names.append((f, f_))\n",
    "\n",
    "print(\"changed_names:\",  len(changed_names))\n",
    "print(\"incorrect_names:\", len(incorrect_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6006-fix-incorrect-names",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try to resolve truly incorrect names via sender_id lookup\n",
    "for f in incorrect_names:\n",
    "    print()\n",
    "    print(f)\n",
    "    sender_id = get_sender_id(f)\n",
    "    print(sender_id)\n",
    "    handle = sender_id2handle.get(sender_id, None)\n",
    "    if handle:\n",
    "        print(f'Renaming \"telegram_messages-{f}.json\" → \"telegram_messages-{handle}.json\"')\n",
    "        try:\n",
    "            os.rename(\n",
    "                f\"../data/Telegram_data/posts/telegram_messages-{f}.json\",\n",
    "                f\"../data/Telegram_data/posts/telegram_messages-{handle}.json\"\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            print(\"already modified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "324545b5-7137-44d3-a8ff-287954d86f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove channels than are not in channels_final_dataset\n",
    "removed_channels = set(channel_filenames) - set(channels_df['channel_url']).union(set(channels_df['username']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "85760c5b-66c2-461b-bd37-957b3d6a032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in removed_channels:\n",
    "    os.rename(\n",
    "                f\"../data/Telegram_data/posts/telegram_messages-{f}.json\",\n",
    "                f\"../data/Telegram_data/posts/removed_channels/telegram_messages-{f}.json\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6007-fix-changed-names",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename files that used a username instead of channel_url\n",
    "for f in changed_names:\n",
    "    print(f)\n",
    "    print(f'Renaming \"telegram_messages-{f[0]}.json\" → \"telegram_messages-{f[1]}.json\"')\n",
    "    try:\n",
    "        os.rename(\n",
    "            f\"../data/Telegram_data/posts/telegram_messages-{f[0]}.json\",\n",
    "            f\"../data/Telegram_data/posts/telegram_messages-{f[1]}.json\"\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(\"already modified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000-merge-posts",
   "metadata": {},
   "source": [
    "## Step 6 - Merge all post JSONs into one file\n",
    "Iterate over all `telegram_messages-*.json` files (TGstat + recommended) and combine them into a single dict keyed by `channel_url`. Each post gets a unique `uid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7001-merge-all-posts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5249393361e43a49039f94386658ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Total posts: 3653769\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\"../data/Telegram_data/posts\")\n",
    "\n",
    "output = {}\n",
    "uid_counter = 1\n",
    "\n",
    "for fname in tqdm(os.listdir(BASE_DIR)):\n",
    "    if not fname.startswith(\"telegram_messages-\") or not fname.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    channel_url = fname.replace(\"telegram_messages-\", \"\").replace(\".json\", \"\")\n",
    "\n",
    "    with open(BASE_DIR / fname, \"r\", encoding=\"utf-8\") as f:\n",
    "        posts = json.load(f, parse_int=str)\n",
    "\n",
    "    if channel_url not in output:\n",
    "        output[channel_url] = []\n",
    "\n",
    "    for post in posts:\n",
    "        post[\"uid\"] = str(uid_counter)\n",
    "        uid_counter += 1\n",
    "        if \"id\" in post:\n",
    "            post[\"id\"] = str(post[\"id\"])\n",
    "        output[channel_url].append(post)\n",
    "\n",
    "with open(\"../data/Telegram_data/all_posts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Done. Total posts: {uid_counter - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000-clean-posts",
   "metadata": {},
   "source": [
    "## Step 7 - Clean post fields\n",
    "Normalise `document_id` / `photo_id` → `media_id` and keep only the relevant columns.\n",
    "\n",
    "**Post schema**\n",
    "| Field | Notes |\n",
    "|---|---|\n",
    "| `id`, `uid`, `sender_id`, `date`, `text` | core fields |\n",
    "| `views`, `forwards`, `reactions`, `comments` | engagement |\n",
    "| `origin_from_id`, `origin_channel_post` | forwarded-post fields |\n",
    "| `media_id`, `media_type`, `size` | media fields |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8001-load-all-posts",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Telegram_data/all_posts.json\") as f:\n",
    "    all_posts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8002-clean-fields",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9707e90cb85d4fa8bd6307b6d0caae7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned posts for 2190 channels\n"
     ]
    }
   ],
   "source": [
    "columns = ['id', 'uid', 'sender_id', 'date', 'text', 'views', 'forwards',\n",
    "           'reactions', 'comments', 'origin_from_id', 'origin_channel_post',\n",
    "           'media_id', 'media_type', 'size']\n",
    "\n",
    "all_posts_final = dict()\n",
    "\n",
    "for channel, posts in tqdm(all_posts.items()):\n",
    "    all_posts_final[channel] = []\n",
    "    for post in posts:\n",
    "        if 'document_id' in post:\n",
    "            post['media_id'] = post['document_id']\n",
    "        if 'photo_id' in post:\n",
    "            post['media_id'] = post['photo_id']\n",
    "        row_dict = {col: post.get(col) for col in columns}\n",
    "        all_posts_final[channel].append(row_dict)\n",
    "\n",
    "with open(\"../data/Telegram_data/all_posts.json\", 'w') as f:\n",
    "    json.dump(all_posts_final, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Cleaned posts for {len(all_posts_final)} channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12000-pseudonymize",
   "metadata": {},
   "source": [
    "## Step 8 - Pseudonymize commenter IDs\n",
    "Replace each real commenter ID with a stable, anonymous integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12001-load-posts-for-pseudo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2190 channels loaded\n"
     ]
    }
   ],
   "source": [
    "#with open('../data/Telegram_data/posts.json') as f:\n",
    "#    all_posts_final = json.load(f)\n",
    "\n",
    "print(f\"{len(all_posts_final)} channels loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "12002-collect-commenter-ids",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e103b56460f4ec19abaf5b4d604eb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments: 3197201\n",
      "Unique commenter IDs: 2159866\n"
     ]
    }
   ],
   "source": [
    "# Collect all commenter IDs\n",
    "commenters_ids = []\n",
    "for k, posts in tqdm(all_posts_final.items()):\n",
    "    for post in posts:\n",
    "        commenters_ids += post['comments']\n",
    "\n",
    "commenters_ids = [c['id'] for c in commenters_ids]\n",
    "print(f\"Total comments: {len(commenters_ids)}\")\n",
    "print(f\"Unique commenter IDs: {len(set(commenters_ids))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "12003-build-pseudo-map",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build stable pseudo-ID mapping (sorted so mapping is reproducible)\n",
    "unique_commenter_ids = sorted(list(set(commenters_ids)))\n",
    "commenters_id2pseudo_id = {c_id: i for i, c_id in enumerate(unique_commenter_ids)}\n",
    "\n",
    "# Save the mapping for reference\n",
    "with open('../data/commenters_id2pseudo_id.json', 'w') as f:\n",
    "    json.dump(commenters_id2pseudo_id, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "12004-apply-pseudonymization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b745f98a6d344c9a49b64b6340c8c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique commenters pseudonymized: 2159866\n"
     ]
    }
   ],
   "source": [
    "# Apply pseudonymization in-place\n",
    "for channel, posts in tqdm(all_posts_final.items(), total=len(all_posts_final)):\n",
    "    posts_new = []\n",
    "    for post in posts:\n",
    "        post_new = dict(post)\n",
    "        comments = post.get(\"comments\") or []\n",
    "        if comments:\n",
    "            new_comments = []\n",
    "            for comment in comments:\n",
    "                c = dict(comment)\n",
    "                orig_id = c.get(\"id\")\n",
    "                if orig_id is not None:\n",
    "                    orig_id = str(orig_id)\n",
    "                    c[\"id\"] = commenters_id2pseudo_id[orig_id]\n",
    "                new_comments.append(c)\n",
    "            post_new[\"comments\"] = new_comments\n",
    "        posts_new.append(post_new)\n",
    "    all_posts_final[channel] = posts_new\n",
    "\n",
    "print(f\"Total unique commenters pseudonymized: {len(commenters_id2pseudo_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "12005-verify-pseudonymization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98ff98db02e49df97923c90d32654b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments:          3197201\n",
      "Unique pseudo-IDs:       2159866\n",
      "All IDs are integers:    True\n"
     ]
    }
   ],
   "source": [
    "# Quick verification: collect pseudo IDs from the updated posts\n",
    "comments_pseudo = []\n",
    "for k, posts in tqdm(all_posts_final.items()):\n",
    "    for post in posts:\n",
    "        comments_pseudo += post['comments']\n",
    "\n",
    "all_ids = [c['id'] for c in comments_pseudo]\n",
    "print(f\"Total comments:          {len(all_ids)}\")\n",
    "print(f\"Unique pseudo-IDs:       {len(set(all_ids))}\")\n",
    "print(f\"All IDs are integers:    {all(isinstance(i, int) for i in all_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "12006-save-pseudonymized",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ posts_pseudo.json saved\n"
     ]
    }
   ],
   "source": [
    "# Save final pseudonymized posts file\n",
    "with open('../data/Telegram_data/posts_pseudo.json', 'w') as f:\n",
    "    json.dump(all_posts_final, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ posts_pseudo.json saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11000-media-hash",
   "metadata": {},
   "source": [
    "## Step 9 - Verify media hash ↔ media_id mapping\n",
    "Each `media_id` should map to exactly one `content_hash`. If a hash has multiple `media_id`s, the same file was re-uploaded to Telegram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11001-load-posts-w-hash",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_w_hash = {}\n",
    "for filename in tqdm(os.listdir(\"api_attribution_media_updated_by_collect/1/\")):\n",
    "    if Path(filename).suffix == \".json\":\n",
    "        with open(Path(\"api_attribution_media_updated_by_collect/1/\") / filename) as f:\n",
    "            channel_postids = json.load(f)\n",
    "        posts_w_hash.update(channel_postids)\n",
    "\n",
    "columns_hash = [\"channel_url\", \"id\", \"mime_type\", \"size_bytes\", \"content_hash\"]\n",
    "rows_hash = []\n",
    "for channel, posts in tqdm(posts_w_hash.items()):\n",
    "    for post in posts:\n",
    "        row_dict = {col: post.get(col) for col in columns_hash}\n",
    "        row_dict[\"channel_url\"] = channel\n",
    "        rows_hash.append(row_dict)\n",
    "\n",
    "posts_w_hash_df = pd.DataFrame(rows_hash)\n",
    "print(posts_w_hash_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11002-subset-posts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all_posts to get the full post metadata for the channels with hashes\n",
    "with open(\"../data/Telegram_data/all_posts.json\") as f:\n",
    "    all_posts = json.load(f)\n",
    "\n",
    "channels_oi = set(posts_w_hash_df.channel_url)\n",
    "subset_posts = {channel: all_posts[channel] for channel in all_posts if channel in channels_oi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11003-build-video-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract video posts from the subset\n",
    "video_cols = ['id', 'has_media', 'media_type', 'media_id']\n",
    "rows_video = []\n",
    "for channel, posts in tqdm(subset_posts.items()):\n",
    "    for post in posts:\n",
    "        if 'document_id' in post:\n",
    "            post['media_id'] = post['document_id']\n",
    "        if 'photo_id' in post:\n",
    "            post['media_id'] = post['photo_id']\n",
    "        row_dict = {col: post.get(col) for col in video_cols}\n",
    "        row_dict[\"channel_url\"] = channel\n",
    "        rows_video.append(row_dict)\n",
    "\n",
    "channels_origin_posts_df = pd.DataFrame(rows_video)\n",
    "channels_origin_posts_df = channels_origin_posts_df[channels_origin_posts_df['media_type'] == 'video'].copy()\n",
    "channels_origin_posts_df['id'] = channels_origin_posts_df['id'].apply(int)\n",
    "posts_w_hash_df['id'] = posts_w_hash_df['id'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11004-merge-hash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and check: how many content_hashes map to more than one media_id?\n",
    "merged = channels_origin_posts_df.merge(posts_w_hash_df, on=['channel_url', 'id'])\n",
    "merged_by_content_hash = merged.groupby('content_hash')['media_id'].unique().reset_index()\n",
    "merged_by_content_hash['n_media_id'] = merged_by_content_hash['media_id'].apply(len)\n",
    "\n",
    "multi = merged_by_content_hash[merged_by_content_hash.n_media_id > 1]\n",
    "print(f\"{len(multi)} hashes map to more than one media_id (same video re-uploaded)\")\n",
    "display(merged_by_content_hash[merged_by_content_hash.n_media_id > 1].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
